{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t2\n",
      "Device:\t\tcuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import ImprovedUnet\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "# Note: using channel-concat IHC conditioning (IHC image -> condition); no label-based ConditionalEmbedding used here.\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",0)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "def createDirectory(directory):\n",
    "    \"\"\"_summary_\n",
    "        create Directory\n",
    "    Args:\n",
    "        directory (string): file_path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Í∑†ÌòïÏû°Ìûå ÏÑ§Ï†ï (8-12GB GPU)\n",
    "params = {\n",
    "    'image_size': 512,\n",
    "    'lr': 2e-5,           # ‚¨ÜÔ∏è Ï¶ùÍ∞Ä\n",
    "    'beta1': 0.9,         # ‚¨ÜÔ∏è Ï¶ùÍ∞Ä\n",
    "    'beta2': 0.999,\n",
    "    'batch_size': 1,      # ‚¨ÜÔ∏è Ï¶ùÍ∞Ä (Î©îÎ™®Î¶¨ ÌóàÏö©Ïãú 4)\n",
    "    'epochs': 1000,\n",
    "    'n_classes': None,\n",
    "    'image_count': 100,\n",
    "    'inch': 3,\n",
    "    'mask_ch': 3,\n",
    "    'modch': 64,\n",
    "    'outch': 3,\n",
    "    'chmul': [1, 2, 4, 8],\n",
    "    'numres': 2,          # ‚¨ÜÔ∏è Ï¶ùÍ∞Ä\n",
    "    'dtype': torch.float32,\n",
    "    'cdim': 10,\n",
    "    'useconv': True,      # ‚¨ÜÔ∏è Î≥ÄÍ≤Ω\n",
    "    'droprate': 0.1,\n",
    "    'T': 1000,\n",
    "    'w': 1.8,\n",
    "    'v': 0.3,\n",
    "    'multiplier': 1,\n",
    "    'threshold': 0.1,\n",
    "    'ddim': True,\n",
    "    'gen_n': 8,\n",
    "    \n",
    "    # ‚≠ê Ï∂îÍ∞Ä Í∂åÏû• ÌååÎùºÎØ∏ÌÑ∞\n",
    "    'use_checkpoint': True,  # Gradient checkpointing\n",
    "    'num_heads': 4,          # Attention heads\n",
    "    'ema_decay': 0.9999,     # EMA (Ï∂îÏ≤ú)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 26.93it/s]\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"IHC->HE paired dataset.\n",
    "\n",
    "    Returns (ihc_image, he_image, label) for each index.\n",
    "    \"\"\"\n",
    "    def __init__(self,parmas, images,mask):\n",
    "        \n",
    "        self.images = images\n",
    "        self.masks = mask\n",
    "        self.args=parmas\n",
    "      \n",
    "        \n",
    "    def trans(self,image,mask):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "        return image,mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "\n",
    "        mask=self.masks[index]\n",
    "        image,mask = self.trans(image,mask)\n",
    "        return image,mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_path=[]\n",
    "ihc_image_list=glob('../../data/IHC4BC_Compressed/**/HER2/IHC/*.jpg')\n",
    "image_temp_list=glob('../../data/IHC4BC_Compressed/**/Ki67/IHC/*.jpg')\n",
    "ihc_image_list.extend(image_temp_list)\n",
    "he_image_list=[p.replace('/IHC/','/HE/') for p in ihc_image_list]\n",
    "if len(ihc_image_list)>params['image_count']:\n",
    "    ihc_image_list=ihc_image_list[:params['image_count']]\n",
    "    he_image_list=he_image_list[:params['image_count']]\n",
    "\n",
    "train_ihc_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "train_he_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "\n",
    "for i in tqdm(range(len(ihc_image_list))):\n",
    "    train_ihc_image[i]=trans(transforms.ToTensor()(Image.open(ihc_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "    train_he_image[i]=trans(transforms.ToTensor()(Image.open(he_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "    \n",
    "train_dataset=CustomDataset(params,train_ihc_image,train_he_image)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],num_workers=4,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ImprovedUnet(in_ch = params['inch'] + params['mask_ch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "# We will sample conditioning IHC images from `train_ihc_image` at generation time (no persistent cond tensor list needed).\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                diffusion.model.parameters(),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-4\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer = optimizer,\n",
    "                            T_max = params['epochs']/100,\n",
    "                            eta_min = 0,\n",
    "                            last_epoch = -1\n",
    "                        )\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = params['epochs'] // 10,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1072664/2658854686.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_1072664/2658854686.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      " 28%|‚ñà‚ñà‚ñä       | 28/100 [00:03<00:09,  7.54it/s, epoch=1, loss=nan, batch per device=1, img shape=torch.Size([3, 512, 512]), LR=2e-5] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     loss = diffusion.trainloss(x_0, mask=cond_input, cemb=cemb)\n\u001b[32m     26\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m scaler.step(optimizer)\n\u001b[32m     29\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/urban/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/urban/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/urban/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epc in range(params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    with tqdm(train_dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, mask in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "\n",
    "            # For image-translation: img = IHC (condition), mask = HE (target)\n",
    "            cond_ihc = img.to(device)\n",
    "            x_0 = mask.to(device)\n",
    "\n",
    "            # Channel-concat conditioning: prepare cond and zero global cemb\n",
    "            cemb = torch.zeros((b, params['cdim']), device=device)\n",
    "            # classifier-free guidance by dropping conditioning images\n",
    "            cond_input = cond_ihc.clone()\n",
    "            drop_idx = (torch.rand(b, device=device) < params['threshold'])\n",
    "            if drop_idx.any():\n",
    "                cond_input[drop_idx] = 0\n",
    "\n",
    "            # AMPÎ•º ÏÇ¨Ïö©Ìïú ÏÜêÏã§ Í≥ÑÏÇ∞ Î∞è Ïó≠Ï†ÑÌåå\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = diffusion.trainloss(x_0, mask=cond_input, cemb=cemb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            steps += 1\n",
    "            total_loss += loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss\": total_loss / steps,\n",
    "                    \"batch per device\": x_0.shape[0],\n",
    "                    \"img shape\": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    warmUpScheduler.step()\n",
    "    diffusion.model.eval()\n",
    "    all_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Select conditioning exemplars from train_ihc_image (first params['gen_n'] or whole if smaller)\n",
    "        try:\n",
    "            gen_n = min(params['gen_n'], train_ihc_image.shape[0])\n",
    "            cond_for_gen = train_ihc_image[:gen_n].to(device)\n",
    "        except NameError:\n",
    "            # If train_ihc_image isn't available, fallback to a random noise cond of appropriate channels\n",
    "            cond_for_gen = torch.randn((params['gen_n'], params['mask_ch'], params['image_size'], params['image_size']), device=device)\n",
    "\n",
    "        # Define generation shape for the image batches\n",
    "        genshape = (cond_for_gen.shape[0], 3, params['image_size'], params['image_size'])\n",
    "        # Sample images using the chosen method (DDIM or standard sampling)\n",
    "        if params['ddim']:\n",
    "            generated = diffusion.ddim_sample(genshape, 50, 0.5, 'quadratic', mask=cond_for_gen, cemb=cemb)\n",
    "        else:\n",
    "            generated = diffusion.sample(genshape, mask=cond_for_gen, cemb=cemb)\n",
    "\n",
    "        # Convert the generated tensors to images and save them (NO Generator postprocessing)\n",
    "        generated = transback(generated.to(device))\n",
    "        concatenated_images = torch.cat([torch.concat([cond_for_gen[i].cpu() / 2. + 0.5, generated[i].cpu() / 2. + 0.5], dim=2) for i in range(cond_for_gen.shape[0])], dim=1)\n",
    "        img_pil = topilimage(concatenated_images)\n",
    "        createDirectory(f'../../results/IHC2HE/condition_diffusion/membrane')\n",
    "        img_pil.save(f'../../results/IHC2HE/condition_diffusion/membrane/epc_{epc}_all_samples.png')\n",
    "\n",
    "        # Save model checkpoint (no cemblayer state needed)\n",
    "        checkpoint = {\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': warmUpScheduler.state_dict()\n",
    "        }\n",
    "        createDirectory(f'../../model/IHC2HE/condition_diffusion/membrane/')\n",
    "        torch.save(checkpoint, f'../../model/IHC2HE/condition_diffusion/membrane/ckpt_{epc+1}_checkpoint.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
