{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t2\n",
      "Device:\t\tcuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding, MaskEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",0)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "def createDirectory(directory):\n",
    "    \"\"\"_summary_\n",
    "        create Directory\n",
    "    Args:\n",
    "        directory (string): file_path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'image_count':20000,\n",
    "        'inch':3,\n",
    "        'mask_ch':3,  # set to number of mask channels (change to len(mask_list)+1 if using one-hot masks)\n",
    "        'modch':128,\n",
    "        'outch':3,\n",
    "        'chmul':[1,2,4,8],\n",
    "        'numres':1,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':1,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     ihc_image_list=ihc_image_list[:params[\u001b[33m'\u001b[39m\u001b[33mimage_count\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     48\u001b[39m     he_image_list=he_image_list[:params[\u001b[33m'\u001b[39m\u001b[33mimage_count\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m train_ihc_image=\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mihc_image_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m train_he_image=torch.zeros((\u001b[38;5;28mlen\u001b[39m(ihc_image_list),params[\u001b[33m'\u001b[39m\u001b[33minch\u001b[39m\u001b[33m'\u001b[39m],params[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m],params[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ihc_image_list))):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,mask,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.masks = mask\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image,mask):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "        return image,mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        mask=self.masks[index]\n",
    "        image,mask = self.trans(image,mask)\n",
    "        return image,mask,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "ihc_image_list=glob('../../data/IHC4BC_Compressed/**/HER2/IHC/*.jpg')\n",
    "image_temp_list=glob('../../data/IHC4BC_Compressed/**/Ki67/IHC/*.jpg')\n",
    "ihc_image_list.extend(image_temp_list)\n",
    "he_image_list=[p.replace('/IHC/','/HE/') for p in ihc_image_list]\n",
    "if len(ihc_image_list)>params['image_count']:\n",
    "    ihc_image_list=ihc_image_list[:params['image_count']]\n",
    "    he_image_list=he_image_list[:params['image_count']]\n",
    "\n",
    "train_ihc_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "train_he_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "\n",
    "for i in tqdm(range(len(ihc_image_list))):\n",
    "    train_ihc_image[i]=trans(transforms.ToTensor()(Image.open(ihc_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "    train_he_image[i]=trans(transforms.ToTensor()(Image.open(he_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using channel-concat conditioning: concatenate x and cond_image along channels and feed to Unet\n",
    "# If you have a mask_list (one-hot masks), set params['mask_ch'] accordingly; otherwise keep the default.\n",
    "\n",
    "\n",
    "net = Unet(in_ch = params['inch'] + params['mask_ch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "# Use image-based mask embedding (image -> global conditional vector)\n",
    "# If your masks have different number of channels (e.g., one-hot masks with shape [num_classes, H, W]),\n",
    "# set in_ch accordingly, e.g., in_ch = len(mask_list)+1\n",
    "cemblayer = MaskEmbedding(in_ch = params['inch'], cdim = params['cdim']).to(device)\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                itertools.chain(\n",
    "                    diffusion.model.parameters(),\n",
    "                    cemblayer.parameters()\n",
    "                ),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-4\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer = optimizer,\n",
    "                            T_max = params['epochs']/100,\n",
    "                            eta_min = 0,\n",
    "                            last_epoch = -1\n",
    "                        )\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = params['epochs'] // 10,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features, in_features, 3),\n",
    "                      nn.InstanceNorm2d(in_features),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features, in_features, 3),\n",
    "                      nn.InstanceNorm2d(in_features)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "\n",
    "\n",
    "# checkpoint=torch.load(f'../../model/conditionDiff/details/STNT/ckpt_231_checkpoint.pt',map_location=device)\n",
    "# diffusion.model.load_state_dict(checkpoint['net'])\n",
    "# cemblayer.load_state_dict(checkpoint['cemblayer'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# warmUpScheduler.load_state_dict(checkpoint['scheduler'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tensor_list=torch.zeros((len(class_list),len(mask_list)+1,params['image_size'],params['image_size'])).to(device)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epc in range(params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    cemblayer.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, mask, lab in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "\n",
    "            x_0 = img.to(device)\n",
    "            mask_0 = mask.to(device)\n",
    "            lab = lab.to(device)\n",
    "\n",
    "            # 조건 임베딩 계산: mask(image) -> cemb\n",
    "            cemb = cemblayer(mask_0)\n",
    "            # classifier-free guidance: randomly drop some conditioning embeddings\n",
    "            drop_idx = (torch.rand(b, device=device) < params['threshold'])\n",
    "            if drop_idx.any():\n",
    "                cemb[drop_idx] = 0\n",
    "            mask_tensor_list[lab.argmax().item()] = mask_0 \n",
    "            # AMP를 사용한 손실 계산 및 역전파\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss =  diffusion.trainloss(x_0, mask=mask_0, cemb=cemb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            steps += 1\n",
    "            total_loss += loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss\": total_loss / steps,\n",
    "                    \"batch per device\": x_0.shape[0],\n",
    "                    \"img shape\": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    warmUpScheduler.step()\n",
    "    diffusion.model.eval()\n",
    "    cemblayer.eval()\n",
    "    all_samples = []\n",
    "        # Use stored masks per class to compute conditional embeddings (image -> cemb)\n",
    "        mask_for_gen = mask_tensor_list.to(device)\n",
    "        cemb = cemblayer(mask_for_gen)\n",
    "        \n",
    "        # Define generation shape for the image batches\n",
    "        genshape = (each_device_batch, 3, params['image_size'], params['image_size'])\n",
    "        # Sample images using the chosen method (DDIM or standard sampling)\n",
    "        if params['ddim']:\n",
    "            generated = diffusion.ddim_sample(genshape, 50, 0.5, 'quadratic', mask=mask_for_gen, cemb=cemb)\n",
    "        else:\n",
    "            generated = diffusion.sample(genshape, mask=mask_for_gen, cemb=cemb)\n",
    "        \n",
    "        # Convert the generated tensors to images and save them\n",
    "        generated = transback(Generator(generated.to(device)).to(device))\n",
    "        for i in range(len(class_list)):\n",
    "            img_pil = topilimage(torch.concat([generated[i].cpu(),(mask_for_gen[i].cpu().argmax(dim=0)/len(mask_list)*2-1).unsqueeze(0).repeat(3, 1, 1)],dim=2))\n",
    "            createDirectory(\n",
    "                f'../../result/mask_Diffusion/BRIL/{class_list[i]}/')\n",
    "            img_pil.save(f'../../result/mask_Diffusion/BRIL/{class_list[i]}/{epc}.png')\n",
    "        \n",
    "        # Save model checkpoints\n",
    "        checkpoint = {\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'cemblayer': cemblayer.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': warmUpScheduler.state_dict()\n",
    "        }\n",
    "        createDirectory(\n",
    "        f'../../model/mask_Diffusion/BRIL/')\n",
    "        torch.save(checkpoint, f'../../model/mask_Diffusion/BRIL/ckpt_{epc+1}_checkpoint.pt')\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'cemblayer': cemblayer.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topilimage(torch.concat([generated[i].cpu(),(mask_tensor_list[0].cpu().argmax(dim=0)/len(mask_list)*2-1).unsqueeze(0).repeat(3, 1, 1)],dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(mask_tensor_list[0].cpu().argmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mask_tensor_list[0].cpu().argmax(dim=0)/len(mask_list)*2-1).unsqueeze(0).repeat(3, 1, 1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
