{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs used:\t2\n",
      "Device:\t\tcuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "# Note: using channel-concat IHC conditioning (IHC image -> condition); no label-based ConditionalEmbedding used here.\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",0)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "def createDirectory(directory):\n",
    "    \"\"\"_summary_\n",
    "        create Directory\n",
    "    Args:\n",
    "        directory (string): file_path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'image_count':20000,\n",
    "        'inch':3,\n",
    "        'mask_ch': 3,  # number of channels in conditioning IHC images (default same as image channels)\n",
    "        'modch':128,\n",
    "        'outch':3,\n",
    "        'chmul':[1,2,4,8],\n",
    "        'numres':1,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':1,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        'gen_n':8,  # number of IHC exemplars to sample for generation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     ihc_image_list=ihc_image_list[:params[\u001b[33m'\u001b[39m\u001b[33mimage_count\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     48\u001b[39m     he_image_list=he_image_list[:params[\u001b[33m'\u001b[39m\u001b[33mimage_count\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m train_ihc_image=\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mihc_image_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m train_he_image=torch.zeros((\u001b[38;5;28mlen\u001b[39m(ihc_image_list),params[\u001b[33m'\u001b[39m\u001b[33minch\u001b[39m\u001b[33m'\u001b[39m],params[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m],params[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ihc_image_list))):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"IHC->HE paired dataset.\n",
    "\n",
    "    Returns (ihc_image, he_image, label) for each index.\n",
    "    \"\"\"\n",
    "    def __init__(self,parmas, images,mask,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.masks = mask\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image,mask):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "        return image,mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        mask=self.masks[index]\n",
    "        image,mask = self.trans(image,mask)\n",
    "        return image,mask,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "ihc_image_list=glob('../../data/IHC4BC_Compressed/**/HER2/IHC/*.jpg')\n",
    "image_temp_list=glob('../../data/IHC4BC_Compressed/**/Ki67/IHC/*.jpg')\n",
    "ihc_image_list.extend(image_temp_list)\n",
    "he_image_list=[p.replace('/IHC/','/HE/') for p in ihc_image_list]\n",
    "if len(ihc_image_list)>params['image_count']:\n",
    "    ihc_image_list=ihc_image_list[:params['image_count']]\n",
    "    he_image_list=he_image_list[:params['image_count']]\n",
    "\n",
    "train_ihc_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "train_he_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "\n",
    "for i in tqdm(range(len(ihc_image_list))):\n",
    "    train_ihc_image[i]=trans(transforms.ToTensor()(Image.open(ihc_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "    train_he_image[i]=trans(transforms.ToTensor()(Image.open(he_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "    \n",
    "train_dataset=CustomDataset(params,train_ihc_image,train_he_image,image_label)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],num_workers=4,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary() got an unexpected keyword argument 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     34\u001b[39m warmUpScheduler = GradualWarmupScheduler(\n\u001b[32m     35\u001b[39m                         optimizer = optimizer,\n\u001b[32m     36\u001b[39m                         multiplier = params[\u001b[33m'\u001b[39m\u001b[33mmultiplier\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m                         last_epoch = \u001b[32m0\u001b[39m\n\u001b[32m     40\u001b[39m                     )\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_model_summary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmask_ch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcdim\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: summary() got an unexpected keyword argument 'inputs'"
     ]
    }
   ],
   "source": [
    "net = Unet(in_ch = params['inch'] + params['mask_ch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "# We will sample conditioning IHC images from `train_ihc_image` at generation time (no persistent cond tensor list needed).\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                diffusion.model.parameters(),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-4\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer = optimizer,\n",
    "                            T_max = params['epochs']/100,\n",
    "                            eta_min = 0,\n",
    "                            last_epoch = -1\n",
    "                        )\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = params['epochs'] // 10,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_model_summary import summary\n",
    "print(summary(net, torch.ones(1, params['inch'] + params['mask_ch'], params['image_size'], params['image_size']), max_depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epc in range(params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    with tqdm(train_dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, mask, lab in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "\n",
    "            # For image-translation: img = IHC (condition), mask = HE (target)\n",
    "            cond_ihc = img.to(device)\n",
    "            x_0 = mask.to(device)\n",
    "            lab = lab.to(device)\n",
    "\n",
    "            # Channel-concat conditioning: prepare cond and zero global cemb\n",
    "            cemb = torch.zeros((b, params['cdim']), device=device)\n",
    "            # classifier-free guidance by dropping conditioning images\n",
    "            cond_input = cond_ihc.clone()\n",
    "            drop_idx = (torch.rand(b, device=device) < params['threshold'])\n",
    "            if drop_idx.any():\n",
    "                cond_input[drop_idx] = 0\n",
    "\n",
    "            # AMP를 사용한 손실 계산 및 역전파\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = diffusion.trainloss(x_0, mask=cond_input, cemb=cemb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            steps += 1\n",
    "            total_loss += loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss\": total_loss / steps,\n",
    "                    \"batch per device\": x_0.shape[0],\n",
    "                    \"img shape\": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    warmUpScheduler.step()\n",
    "    diffusion.model.eval()\n",
    "    all_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Select conditioning exemplars from train_ihc_image (first params['gen_n'] or whole if smaller)\n",
    "        try:\n",
    "            gen_n = min(params['gen_n'], train_ihc_image.shape[0])\n",
    "            cond_for_gen = train_ihc_image[:gen_n].to(device)\n",
    "        except NameError:\n",
    "            # If train_ihc_image isn't available, fallback to a random noise cond of appropriate channels\n",
    "            cond_for_gen = torch.randn((params['gen_n'], params['mask_ch'], params['image_size'], params['image_size']), device=device)\n",
    "\n",
    "        # Define generation shape for the image batches\n",
    "        genshape = (cond_for_gen.shape[0], 3, params['image_size'], params['image_size'])\n",
    "        # Sample images using the chosen method (DDIM or standard sampling)\n",
    "        if params['ddim']:\n",
    "            generated = diffusion.ddim_sample(genshape, 50, 0.5, 'quadratic', mask=cond_for_gen, cemb=cemb)\n",
    "        else:\n",
    "            generated = diffusion.sample(genshape, mask=cond_for_gen, cemb=cemb)\n",
    "\n",
    "        # Convert the generated tensors to images and save them (NO Generator postprocessing)\n",
    "        generated = transback(generated.to(device))\n",
    "        for i in range(cond_for_gen.shape[0]):\n",
    "            # show conditioning IHC (left) and generated HE (right)\n",
    "            img_pil = topilimage(torch.concat([cond_for_gen[i].cpu(), generated[i].cpu()], dim=2))\n",
    "            createDirectory(f'../../results/IHC2HE/condition_diffusion/membrain')\n",
    "            img_pil.save(f'../../results/IHC2HE/condition_diffusion/membrain/epc_{epc}_idx_{i}.png')\n",
    "\n",
    "        # Save model checkpoint (no cemblayer state needed)\n",
    "        checkpoint = {\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': warmUpScheduler.state_dict()\n",
    "        }\n",
    "        createDirectory(f'../../model/IHC2HE/condition_diffusion/membrain/')\n",
    "        torch.save(checkpoint, f'../../model/IHC2HE/condition_diffusion/membrain/ckpt_{epc+1}_checkpoint.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
