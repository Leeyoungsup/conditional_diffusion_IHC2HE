{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import UnetWithMask\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding, MaskEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",0)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "def createDirectory(directory):\n",
    "    \"\"\"_summary_\n",
    "        create Directory\n",
    "    Args:\n",
    "        directory (string): file_path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':1,\n",
    "        'epochs':1000,\n",
    "        'n_classes':None,\n",
    "        'image_count':20000,\n",
    "        'inch':3,\n",
    "        'modch':128,\n",
    "        'outch':3,\n",
    "        'chmul':[1,2,4,8],\n",
    "        'numres':1,\n",
    "        'dtype':torch.float32,\n",
    "        'cdim':10,\n",
    "        'useconv':False,\n",
    "        'droprate':0.1,\n",
    "        'T':1000,\n",
    "        'w':1.8,\n",
    "        'v':0.3,\n",
    "        'multiplier':1,\n",
    "        'threshold':0.1,\n",
    "        'ddim':True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,mask,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.masks = mask\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image,mask):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            mask = transform(mask)\n",
    "        return image,mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        mask=self.masks[index]\n",
    "        image,mask = self.trans(image,mask)\n",
    "        return image,mask,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "ihc_image_list=glob('../../data/IHC4BC_Compressed/**/HER2/IHC/*.jpg')\n",
    "image_temp_list=glob('../../data/IHC4BC_Compressed/**/Ki67/IHC/*.jpg')\n",
    "ihc_image_list.extend(image_temp_list)\n",
    "he_image_list=[p.replace('/IHC/','/HE/') for p in ihc_image_list]\n",
    "if len(ihc_image_list)>params['image_count']:\n",
    "    ihc_image_list=ihc_image_list[:params['image_count']]\n",
    "    he_image_list=he_image_list[:params['image_count']]\n",
    "\n",
    "train_ihc_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "train_he_image=torch.zeros((len(ihc_image_list),params['inch'],params['image_size'],params['image_size']))\n",
    "\n",
    "for i in tqdm(range(len(ihc_image_list))):\n",
    "    train_ihc_image[i]=trans(transforms.ToTensor()(Image.open(ihc_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))\n",
    "    train_he_image[i]=trans(transforms.ToTensor()(Image.open(he_image_list[i]).convert('RGB').resize((params['image_size'],params['image_size']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UnetWithMask(in_ch = params['inch'],\n",
    "                   mask_ch=params['inch'],\n",
    "            mod_ch = params['modch'],\n",
    "            out_ch = params['outch'],\n",
    "            ch_mul = params['chmul'],\n",
    "            num_res_blocks = params['numres'],\n",
    "            cdim = params['cdim'],\n",
    "            use_conv = params['useconv'],\n",
    "            droprate = params['droprate'],\n",
    "            dtype = params['dtype']\n",
    "            ).to(device)\n",
    "# Use image-based mask embedding (image -> global conditional vector)\n",
    "# If your masks have different number of channels (e.g., one-hot masks with shape [num_classes, H, W]),\n",
    "# set in_ch accordingly, e.g., in_ch = len(mask_list)+1\n",
    "cemblayer = MaskEmbedding(in_ch = params['inch'], cdim = params['cdim']).to(device)\n",
    "betas = get_named_beta_schedule(num_diffusion_timesteps = params['T'])\n",
    "diffusion = GaussianDiffusion(\n",
    "                    dtype = params['dtype'],\n",
    "                    model = net,\n",
    "                    betas = betas,\n",
    "                    w = params['w'],\n",
    "                    v = params['v'],\n",
    "                    device = device\n",
    "                )\n",
    "optimizer = torch.optim.AdamW(\n",
    "                itertools.chain(\n",
    "                    diffusion.model.parameters(),\n",
    "                    cemblayer.parameters()\n",
    "                ),\n",
    "                lr = params['lr'],\n",
    "                weight_decay = 1e-4\n",
    "            )\n",
    "\n",
    "\n",
    "cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer = optimizer,\n",
    "                            T_max = params['epochs']/100,\n",
    "                            eta_min = 0,\n",
    "                            last_epoch = -1\n",
    "                        )\n",
    "warmUpScheduler = GradualWarmupScheduler(\n",
    "                        optimizer = optimizer,\n",
    "                        multiplier = params['multiplier'],\n",
    "                        warm_epoch = params['epochs'] // 10,\n",
    "                        after_scheduler = cosineScheduler,\n",
    "                        last_epoch = 0\n",
    "                    )\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features, in_features, 3),\n",
    "                      nn.InstanceNorm2d(in_features),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features, in_features, 3),\n",
    "                      nn.InstanceNorm2d(in_features)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, 64, 7),\n",
    "                 nn.InstanceNorm2d(64),\n",
    "                 nn.ReLU(inplace=True)]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                      nn.InstanceNorm2d(out_features),\n",
    "                      nn.ReLU(inplace=True)]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                      nn.InstanceNorm2d(out_features),\n",
    "                      nn.ReLU(inplace=True)]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(3),\n",
    "                  nn.Conv2d(64, output_nc, 7),\n",
    "                  nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "Generator = Generator(3, 3).to(device)\n",
    "Generator.load_state_dict(torch.load(\n",
    "    '../../model/cyclegan/G_B_29.pth', map_location=device))\n",
    "\n",
    "# checkpoint=torch.load(f'../../model/conditionDiff/details/STNT/ckpt_231_checkpoint.pt',map_location=device)\n",
    "# diffusion.model.load_state_dict(checkpoint['net'])\n",
    "# cemblayer.load_state_dict(checkpoint['cemblayer'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# warmUpScheduler.load_state_dict(checkpoint['scheduler'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tensor_list=torch.zeros((len(class_list),len(mask_list)+1,params['image_size'],params['image_size'])).to(device)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epc in range(params['epochs']):\n",
    "    diffusion.model.train()\n",
    "    cemblayer.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, mask, lab in tqdmDataLoader:\n",
    "            b = img.shape[0]\n",
    "\n",
    "            x_0 = img.to(device)\n",
    "            mask_0 = mask.to(device)\n",
    "            lab = lab.to(device)\n",
    "\n",
    "            # 조건 임베딩 계산: mask(image) -> cemb\n",
    "            cemb = cemblayer(mask_0)\n",
    "            # classifier-free guidance: randomly drop some conditioning embeddings\n",
    "            drop_idx = (torch.rand(b, device=device) < params['threshold'])\n",
    "            if drop_idx.any():\n",
    "                cemb[drop_idx] = 0\n",
    "            mask_tensor_list[lab.argmax().item()] = mask_0 \n",
    "            # AMP를 사용한 손실 계산 및 역전파\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss =  diffusion.trainloss(x_0, mask=mask_0, cemb=cemb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            steps += 1\n",
    "            total_loss += loss.item()\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"loss\": total_loss / steps,\n",
    "                    \"batch per device\": x_0.shape[0],\n",
    "                    \"img shape\": x_0.shape[1:],\n",
    "                    \"LR\": optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    warmUpScheduler.step()\n",
    "    diffusion.model.eval()\n",
    "    cemblayer.eval()\n",
    "    all_samples = []\n",
    "        # Use stored masks per class to compute conditional embeddings (image -> cemb)\n",
    "        mask_for_gen = mask_tensor_list.to(device)\n",
    "        cemb = cemblayer(mask_for_gen)\n",
    "        \n",
    "        # Define generation shape for the image batches\n",
    "        genshape = (each_device_batch, 3, params['image_size'], params['image_size'])\n",
    "        # Sample images using the chosen method (DDIM or standard sampling)\n",
    "        if params['ddim']:\n",
    "            generated = diffusion.ddim_sample(genshape, 50, 0.5, 'quadratic', mask=mask_for_gen, cemb=cemb)\n",
    "        else:\n",
    "            generated = diffusion.sample(genshape, mask=mask_for_gen, cemb=cemb)\n",
    "        \n",
    "        # Convert the generated tensors to images and save them\n",
    "        generated = transback(Generator(generated.to(device)).to(device))\n",
    "        for i in range(len(class_list)):\n",
    "            img_pil = topilimage(torch.concat([generated[i].cpu(),(mask_for_gen[i].cpu().argmax(dim=0)/len(mask_list)*2-1).unsqueeze(0).repeat(3, 1, 1)],dim=2))\n",
    "            createDirectory(\n",
    "                f'../../result/mask_Diffusion/BRIL/{class_list[i]}/')\n",
    "            img_pil.save(f'../../result/mask_Diffusion/BRIL/{class_list[i]}/{epc}.png')\n",
    "        \n",
    "        # Save model checkpoints\n",
    "        checkpoint = {\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'cemblayer': cemblayer.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': warmUpScheduler.state_dict()\n",
    "        }\n",
    "        createDirectory(\n",
    "        f'../../model/mask_Diffusion/BRIL/')\n",
    "        torch.save(checkpoint, f'../../model/mask_Diffusion/BRIL/ckpt_{epc+1}_checkpoint.pt')\n",
    "            'net': diffusion.model.state_dict(),\n",
    "            'cemblayer': cemblayer.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topilimage(torch.concat([generated[i].cpu(),(mask_tensor_list[0].cpu().argmax(dim=0)/len(mask_list)*2-1).unsqueeze(0).repeat(3, 1, 1)],dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(mask_tensor_list[0].cpu().argmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mask_tensor_list[0].cpu().argmax(dim=0)/len(mask_list)*2-1).unsqueeze(0).repeat(3, 1, 1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
